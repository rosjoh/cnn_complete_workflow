{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from osgeo import gdal, ogr\n",
    "\n",
    "%matplotlib inline\n",
    "tf.enable_eager_execution()\n",
    "tfe = tf.contrib.eager\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.VERSION))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "print(\"GPU available:\", tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "#area = \"berlin\"\n",
    "#area = \"hamburg\"\n",
    "#area = \"frankfurt\"\n",
    "area = \"cologne\"\n",
    "#area = \"munich\"\n",
    "#area = \"rostock\"\n",
    "#area = \"muenster\"\n",
    "#area = \"dortmund\"\n",
    "\n",
    "os.chdir(\"/path/to/your/working/directory\")\n",
    "outDir = os.path.join(os.getcwd(), \"output\")\n",
    "img = os.path.join(os.getcwd(), \"input\", area + \"_omega.vrt\")\n",
    "shpTr = os.path.join(os.getcwd(), \"input\", area + \"_train.kmz\")\n",
    "shpTe = os.path.join(os.getcwd(), \"input\", area + \"_test.kmz\")\n",
    "patTr = os.path.join(outDir, \"patches\", area, \"train\")\n",
    "patTe = os.path.join(outDir, \"patches\", area, \"test\")\n",
    "ckptDir = os.path.join(outDir, \"checkpts\", area)\n",
    "ckptPrefix = os.path.join(ckptDir, \"ckpt\")\n",
    "\n",
    "tb = \"/home/user/miniconda3/envs/tf/bin/tensorboard\"\n",
    "logDir = os.path.join(outDir, \"logs\", area)\n",
    "\n",
    "shpAttName = \"Name\"\n",
    "\n",
    "sampleDistance = 2\n",
    "maxPatchesPerClass_train = 12500\n",
    "maxPatchesPerClass_valid = 1000\n",
    "patchSize = 32\n",
    "\n",
    "numBands = 10\n",
    "epochs = 200\n",
    "batchSize = 400\n",
    "learningRate = 0.001\n",
    "dropoutRate = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vrt \n",
    "def createVRT():\n",
    "    outputname = os.path.join(os.getcwd(), \"input\", area + \"_omega.vrt\")\n",
    "    pathy = os.path.join(os.getcwd(), \"data\", area)\n",
    "\n",
    "    bands = []\n",
    "    for file in os.listdir(pathy):\n",
    "        bands.append(os.path.join(pathy, file))\n",
    "    bands.sort()\n",
    "\n",
    "    gdal.BuildVRT(destName = outputname, \n",
    "                  srcDSOrSrcDSTab = bands, \n",
    "                  separate = True\n",
    "                 ).FlushCache()\n",
    "\n",
    "def reprojectVRT():\n",
    "    ds = gdal.Open(img)\n",
    "    ds = gdal.Translate(img[:-4]+\"_reproj.vrt\", ds, options = gdal.TranslateOptions(outputSRS=\"EPSG:32632\"))\n",
    "    ds = None\n",
    "    \n",
    "createVRT()\n",
    "reprojectVRT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rasterize vector files (e.g., shapefile, kmz, ...)\n",
    "if not os.path.exists(patTr):\n",
    "    os.makedirs(patTr)\n",
    "    os.makedirs(patTe)\n",
    "if not os.path.exists(ckptDir):\n",
    "    os.makedirs(ckptDir)\n",
    "    \n",
    "def rasterizeShapefile(imgPath, shpPath):\n",
    "    img = gdal.Open(imgPath)\n",
    "    shp = ogr.Open(shpPath)\n",
    "    out = gdal.GetDriverByName('GTiff')\n",
    "    out = out.Create(os.path.join(outDir, os.path.basename(shpPath[:-4]) + \".tif\"), img.RasterXSize, img.RasterYSize, 1, gdal.GDT_Byte)\n",
    "    out.SetGeoTransform(img.GetGeoTransform())\n",
    "    out.SetProjection(img.GetProjectionRef())\n",
    "    gdal.RasterizeLayer(out, [1], shp.GetLayer(), options=[str(\"ATTRIBUTE=\" + shpAttName)])\n",
    "    r = None\n",
    "\n",
    "rasterizeShapefile(img, shpTr)\n",
    "rasterizeShapefile(img, shpTe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create image patches\n",
    "def createPatches(shpPath, outPath, training):\n",
    "    r = gdal.Open(os.path.join(outDir, os.path.basename(shpPath[:-4]) + \".tif\"))\n",
    "    rData = r.ReadAsArray()\n",
    "    sampleGrid = np.zeros((rData.shape[0], rData.shape[1]))\n",
    "    sampleGrid[int(patchSize/2+1):rData.shape[0]-int(patchSize/2+1):sampleDistance, int(patchSize/2+1):rData.shape[1]-int(patchSize/2+1):sampleDistance] = 1\n",
    "    print(\"unique values:\", np.unique(rData))\n",
    "    for cla in np.unique(rData):\n",
    "        if cla > 0:\n",
    "            xy = np.argwhere(rData == cla)\n",
    "            xy = xy[np.random.permutation(xy.shape[0])]\n",
    "            counter = 0\n",
    "            if training:\n",
    "                for idx, loc in enumerate(xy):\n",
    "                    if sampleGrid[loc[0], loc[1]] == 1 and counter < maxPatchesPerClass_train:\n",
    "                        counter += 1\n",
    "                        out = os.path.join(outPath, str('{0:02d}').format(cla) + \"_\" + area + \"_\" + \\\n",
    "                                           str('{0:0'+str(len(str(maxPatchesPerClass_train)))+'d}').format(counter) + \".vrt\")\n",
    "                        gdal.Translate(out, img, format = 'vrt', srcWin = [loc[1] - patchSize / 2,\n",
    "                                                                             loc[0] - patchSize / 2,\n",
    "                                                                             patchSize,\n",
    "                                                                             patchSize])\n",
    "            else:\n",
    "                for idx, loc in enumerate(xy):\n",
    "                    if counter < maxPatchesPerClass_valid:\n",
    "                        counter += 1\n",
    "                        out = os.path.join(outPath, str('{0:02d}').format(cla) + \"_\" + area + \"_\" + \\\n",
    "                                           str('{0:0'+str(len(str(maxPatchesPerClass_valid)))+'d}').format(counter) + \".vrt\")\n",
    "                        gdal.Translate(out, img, format = 'vrt', srcWin = [loc[1] - patchSize / 2,\n",
    "                                                                             loc[0] - patchSize / 2,\n",
    "                                                                             patchSize,\n",
    "                                                                             patchSize])\n",
    "            print(\"class\", str(cla), \":\", str(counter))\n",
    "                    \n",
    "print(\"num train patches\")\n",
    "createPatches(shpTr, patTr, True)\n",
    "\n",
    "print(\"\\nnum test patches\")\n",
    "createPatches(shpTe, patTe, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image patches\n",
    "def loadPatches(patchFolder):\n",
    "    patches, labels = [], []\n",
    "    for i in os.listdir(patchFolder):\n",
    "        x = gdal.Open(os.path.join(patchFolder, i)).ReadAsArray()\n",
    "        x = np.moveaxis(x, 0, -1)\n",
    "        if not np.isnan(x).any():\n",
    "            patches.append(x)\n",
    "            labels.append(int(i.split(\"_\")[0]))\n",
    "    labels = np.array(labels, dtype=np.int32)\n",
    "    labs = np.zeros(labels.shape, dtype=np.int32)\n",
    "    for idx, c in enumerate(np.unique(labels)):\n",
    "        labs[labels == c] = idx\n",
    "    return np.array(patches, dtype=np.float32), tf.one_hot(labs, len(np.unique(labels))).numpy()\n",
    "\n",
    "def calcNormStats(patches):\n",
    "    means, stds, mins, maxs = [], [], [], []\n",
    "    for i in range(patches.shape[3]):\n",
    "        means.append(np.nanmean(patches[:, :, :, i]))\n",
    "        stds.append(np.nanstd(patches[:, :, :, i]))\n",
    "    np.savetxt(os.path.join(outDir, area + \"_means.txt\"), means)\n",
    "    np.savetxt(os.path.join(outDir, area + \"_stds.txt\"), stds)\n",
    "    return means, stds\n",
    "\n",
    "def showCount(labels):\n",
    "    imp = []\n",
    "    for idx, c in enumerate(range(labels.shape[1])):\n",
    "        imp.append(int(np.sum(labels[:, c])))\n",
    "        print(\"Class:\", idx, \"-\", int(np.sum(labels[:, c])))\n",
    "    weights = tf.nn.softmax(1 - (np.array(imp) / np.sum(np.array(imp)))).numpy()\n",
    "    return tf.constant(list(weights), dtype=tf.float32)\n",
    "\n",
    "def normPatches(patches, means, stds):\n",
    "    for i in range(patches.shape[0]):\n",
    "        patch = patches[i, :, :, :]\n",
    "        for j in range(patch.shape[2]):\n",
    "            patch[:, :, j] = (patch[:, :, j] - means[j]) / stds[j]\n",
    "        patches[i, :, :, :] = patch\n",
    "    return patches\n",
    "\n",
    "# import and normalize train/test data\n",
    "X_train, y_train = loadPatches(patTr)\n",
    "X_test, y_test = loadPatches(patTe)\n",
    "print(\"finished loading\")\n",
    "means, stds = calcNormStats(X_train)\n",
    "print(\"finished calculating\")\n",
    "classweights = showCount(y_train)\n",
    "print(classweights)\n",
    "showCount(y_test)\n",
    "X_train = normPatches(X_train, means, stds)\n",
    "X_test = normPatches(X_test, means, stds)\n",
    "print(\"finished normalizing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets for iterators\n",
    "train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(10000000).batch(batchSize)\n",
    "test_data = tf.data.Dataset.from_tensor_slices((X_test, y_test)).shuffle(10000000).batch(batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, numClasses, dropoutRate, ckptDir, logDir):\n",
    "        super(CNN, self).__init__()\n",
    "        self.ckptDir = ckptDir\n",
    "        self.logDir = logDir\n",
    "        \n",
    "        self.conv1 = tf.layers.Conv2D(filters=16, kernel_size=3, padding=\"same\", activation=None, kernel_initializer=tf.random_normal_initializer(), bias_initializer=tf.random_normal_initializer(), name=\"conv1\")\n",
    "        self.batch1 = tf.layers.BatchNormalization(beta_initializer=tf.random_normal_initializer(), gamma_initializer=tf.random_normal_initializer(), name=\"batch1\")\n",
    "        self.maxpool1 = tf.layers.MaxPooling2D(pool_size=[2, 2], strides=2, padding=\"same\", name=\"maxpool1\")\n",
    "        self.conv2 = tf.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=None, kernel_initializer=tf.random_normal_initializer(), bias_initializer=tf.random_normal_initializer(), name=\"conv2\")\n",
    "        self.batch2 = tf.layers.BatchNormalization(beta_initializer=tf.random_normal_initializer(), gamma_initializer=tf.random_normal_initializer(), name=\"batch2\")\n",
    "        self.maxpool2 = tf.layers.MaxPooling2D(pool_size=[2, 2], strides=2, padding=\"same\", name=\"maxpool2\")\n",
    "        self.conv3 = tf.layers.Conv2D(filters=64, kernel_size=3, padding=\"same\", activation=None, kernel_initializer=tf.random_normal_initializer(), bias_initializer=tf.random_normal_initializer(), name=\"conv3\")\n",
    "        self.batch3 = tf.layers.BatchNormalization(beta_initializer=tf.random_normal_initializer(), gamma_initializer=tf.random_normal_initializer(), name=\"batch3\")\n",
    "        self.maxpool3 = tf.layers.MaxPooling2D(pool_size=[2, 2], strides=2, padding=\"same\", name=\"maxpool3\")\n",
    "        self.conv4 = tf.layers.Conv2D(filters=128, kernel_size=3, padding=\"same\", activation=None, kernel_initializer=tf.random_normal_initializer(), bias_initializer=tf.random_normal_initializer(), name=\"conv4\")\n",
    "        self.batch4 = tf.layers.BatchNormalization(beta_initializer=tf.random_normal_initializer(), gamma_initializer=tf.random_normal_initializer(), name=\"batch4\")\n",
    "        self.maxpool4 = tf.layers.MaxPooling2D(pool_size=[2, 2], strides=2, padding=\"same\", name=\"maxpool4\")\n",
    "        self.dense1 = tf.layers.Dense(units=256, activation=None, kernel_initializer=tf.random_normal_initializer(), bias_initializer=tf.random_normal_initializer(), name=\"dense1\")\n",
    "        self.drop1 = tf.layers.Dropout(rate=dropoutRate, name=\"drop1\")\n",
    "        self.dense2 = tf.layers.Dense(units=numClasses, activation=None, kernel_initializer=tf.random_normal_initializer(), bias_initializer=tf.random_normal_initializer(), name=\"dense2\")\n",
    "\n",
    "    def predict(self, images, phaseTrain):\n",
    "        conv1 = self.conv1(images)\n",
    "        batch1 = self.batch1(conv1, training=phaseTrain)\n",
    "        relu1 = tf.nn.relu(batch1)\n",
    "        pool1 = self.maxpool1(relu1)\n",
    "        conv2 = self.conv2(pool1)\n",
    "        batch2 = self.batch2(conv2, training=phaseTrain)\n",
    "        relu2 = tf.nn.relu(batch2)\n",
    "        pool2 = self.maxpool2(relu2)\n",
    "        conv3 = self.conv3(pool2)\n",
    "        batch3 = self.batch3(conv3, training=phaseTrain)\n",
    "        relu3 = tf.nn.relu(batch3)\n",
    "        pool3 = self.maxpool3(relu3)\n",
    "        conv4 = self.conv4(pool3)\n",
    "        batch4 = self.batch4(conv4, training=phaseTrain)\n",
    "        relu4 = tf.nn.relu(batch4)\n",
    "        pool4 = self.maxpool4(relu4)\n",
    "        flat1 = tf.layers.flatten(pool4)\n",
    "        dense1 = self.dense1(flat1)\n",
    "        relu5 = tf.nn.relu(dense1)\n",
    "        drop1 = self.drop1(relu5, training=phaseTrain)\n",
    "        dense2 = self.dense2(drop1)\n",
    "        return dense2\n",
    "\n",
    "    def augment(self, images, labels):\n",
    "        X, y = [], []\n",
    "        labels = labels.numpy()\n",
    "        images = images.numpy()\n",
    "        for idx, image in enumerate(images):\n",
    "            for r in range(0, 4):\n",
    "                # rotate\n",
    "                imageRotated = np.rot90(image, k=r, axes=(0, 1))\n",
    "                X.append(imageRotated)\n",
    "                y.append(labels[idx, :])\n",
    "                 # flip rotated \n",
    "                imageRotated = np.fliplr(imageRotated)\n",
    "                X.append(imageRotated)\n",
    "                y.append(labels[idx, :])\n",
    "        return tf.convert_to_tensor(np.array(X, np.float32)), tf.convert_to_tensor(np.array(y, np.float32))\n",
    "        \n",
    "    def train(self, trainingData, testData, optimizer, epochs, global_step):  \n",
    "        \n",
    "        time = datetime.datetime.now().strftime(\"%Y%m%d_%H_%M\")\n",
    "        train_summary_writer = tf.contrib.summary.create_file_writer(os.path.join(self.logDir, \"train\", time), name=\"train\")\n",
    "        test_summary_writer = tf.contrib.summary.create_file_writer(os.path.join(self.logDir, \"test\", time), name=\"test\")\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            with train_summary_writer.as_default():\n",
    "                avg_loss_tr = tfe.metrics.Mean(\"train_loss\", dtype=tf.float32)\n",
    "                accuracy_tr = tfe.metrics.Accuracy(\"train_accuracy\", dtype=tf.float32)\n",
    "                for images, labels in tfe.Iterator(trainingData):\n",
    "                    images, labels = self.augment(images, labels)\n",
    "                    with tfe.GradientTape() as tape:\n",
    "                        logits = self.predict(images, True)\n",
    "                        loss = tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=labels, weights=tf.reduce_sum(classweights * labels, axis=1))\n",
    "                    grads = tape.gradient(loss, self.variables)\n",
    "                    optimizer.apply_gradients(zip(grads, self.variables), global_step=global_step)\n",
    "                    avg_loss_tr(loss)\n",
    "                    accuracy_tr(tf.argmax(tf.nn.softmax(logits), axis=1, output_type=tf.int64), tf.argmax(tf.cast(labels, tf.int64), axis=1, output_type=tf.int64))\n",
    "                trainAcc, trainLoss = 100 * accuracy_tr.result(), avg_loss_tr.result()\n",
    "                with tf.contrib.summary.always_record_summaries():\n",
    "                    tf.contrib.summary.image(\"train_images\", images[:16, :, :, :3], max_images=12) \n",
    "                    tf.contrib.summary.scalar(\"train_loss\", avg_loss_tr.result())\n",
    "                    tf.contrib.summary.scalar(\"train_accuracy\", accuracy_tr.result())\n",
    "                        \n",
    "            with test_summary_writer.as_default():   \n",
    "                avg_loss_te = tfe.metrics.Mean(\"test_loss\", dtype=tf.float32)\n",
    "                accuracy_te = tfe.metrics.Accuracy(\"test_accuracy\", dtype=tf.float32)\n",
    "                for images, labels in tfe.Iterator(testData):\n",
    "                    logits = self.predict(images, False)\n",
    "                    avg_loss_te(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=labels))\n",
    "                    accuracy_te(tf.argmax(tf.nn.softmax(logits), axis=1, output_type=tf.int64), tf.argmax(tf.cast(labels, tf.int64), axis=1, output_type=tf.int64))\n",
    "                testAcc, testLoss = 100 * accuracy_te.result(), avg_loss_te.result()\n",
    "                with tf.contrib.summary.always_record_summaries():\n",
    "                    tf.contrib.summary.scalar(\"test_loss\", avg_loss_te.result())\n",
    "                    tf.contrib.summary.scalar(\"test_accuracy\", accuracy_te.result())\n",
    "                        \n",
    "            print(str(\"{0:0\"+str(len(str(epochs)))+\"d}\").format(i+1) + \"/\" + str(epochs) + \"   %.2f   %.2f   %.5f   %.5f\" % (trainAcc.numpy(), testAcc.numpy(), trainLoss.numpy(), testLoss.numpy()))\n",
    "                \n",
    "            if i % 10 == 0:\n",
    "                checkpoint.save(file_prefix=ckptPrefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and load checkpoint\n",
    "model = CNN(y_train.shape[1], dropoutRate, ckptDir, logDir)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learningRate)\n",
    "checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
    "global_step = tf.train.get_or_create_global_step().assign(0)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(ckptDir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "model.train(train_data, test_data, optimizer, epochs, global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image classification\n",
    "imgDataset = gdal.Open(img)\n",
    "imgData = imgDataset.ReadAsArray().astype(np.float32)\n",
    "\n",
    "for i in range(len(means)):\n",
    "    imgData[i, :, :] = (imgData[i, :, :] - means[i]) / stds[i]\n",
    "    \n",
    "classMap = np.zeros((imgData.shape[1], imgData.shape[2]))\n",
    "\n",
    "for x in range(0, imgData.shape[1]-patchSize):\n",
    "    Xpatches = []\n",
    "    for y in range(0, imgData.shape[2]-patchSize):\n",
    "        Xpatch = imgData[:, x:(x+patchSize), y:(y+patchSize)]\n",
    "        Xpatch = np.moveaxis(Xpatch, 0, -1)\n",
    "        Xpatches.append(Xpatch)\n",
    "    Xpatches = np.array(Xpatches, dtype=np.float32)\n",
    "    pred = model.predict(Xpatches, False)\n",
    "    classMap[x+int(patchSize/2), int(patchSize/2):int(patchSize/2)+pred.shape[0]] = np.argmax(tf.nn.softmax(pred), 1)\n",
    "    sys.stdout.write('\\rProgress ' + str(\"{0:.2f}\".format(round(x / (imgData.shape[1]-patchSize)*100, 2))) + \"%\")\n",
    "print(np.unique(classMap))\n",
    "    \n",
    "# export\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "df = driver.Create(os.path.join(outDir, area + \"_classification.tif\"), imgDataset.RasterXSize, imgDataset.RasterYSize, 1, gdal.GDT_Byte)\n",
    "df.SetGeoTransform(imgDataset.GetGeoTransform())\n",
    "df.SetProjection(imgDataset.GetProjectionRef())\n",
    "df.GetRasterBand(1).WriteArray(classMap)\n",
    "df.FlushCache()\n",
    "df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "  \n",
    "classes = np.array((\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"))\n",
    "colors = np.array((\"#8c0000\", # 1\n",
    "                   \"#d00000\", # 2\n",
    "                   \"#ff0000\", # 3\n",
    "                   \"#bf4d00\", # 4\n",
    "                   \"#fe6500\", # 5\n",
    "                   \"#ff9955\", # 6 \n",
    "                   \"#f9ee05\", # 7\n",
    "                   \"#bcbcbc\", # 8\n",
    "                   \"#fecca9\", # 9\n",
    "                   \"#545454\", # 10\n",
    "                   \"#006a00\", # A\n",
    "                   \"#00aa00\", # B\n",
    "                   \"#638425\", # C\n",
    "                   \"#b9db79\", # D\n",
    "                   \"#000000\", # E\n",
    "                   \"#fbf6ae\", # F\n",
    "                   \"#6a6aff\"))# G\n",
    "    \n",
    "    \n",
    "classes = classes[[1, 3, 4, 5, 7, 8, 9, 10, 11, 13, 14, 15, 16]]\n",
    "colors = colors[[1, 3, 4, 5, 7, 8, 9, 10, 11, 13, 14, 15, 16]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classMap = gdal.Open(os.path.join(outDir, area + \"_classification.tif\")).ReadAsArray()\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(classMap, cmap = plt.matplotlib.colors.ListedColormap(colors))\n",
    "patches = [mpatches.Patch(color=colors[i], label=classes[i]) for i in range(len(classes))]\n",
    "plt.legend(handles=patches, loc=\"upper center\", ncol=len(classes), bbox_to_anchor=(0.5, -0.05), prop={\"size\": 12})\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compact highrise  1  - \n",
    "# compact midrise   2  -  1\n",
    "# compact lowrise   3  - \n",
    "# open highrise     4  -  2\n",
    "# open midrise      5  -  3\n",
    "# open lowrise      6  -  4\n",
    "# leigthweight low  7  -  \n",
    "# large lowrise     8  -  5\n",
    "# sparsely built    9  -  6\n",
    "# heavy industry   10  -  7\n",
    "# dense trees      11  -  8 \n",
    "# scattered trees  12  -  9\n",
    "# bush, shrub      13  -  \n",
    "# low plants       14  - 10\n",
    "# bare rock        15  - 11\n",
    "# bare soil        16  - 12\n",
    "# water            17  - 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# area adjusted acc (Olofson)\n",
    "def areaAdjAcc(confmat, classification, classNames, resolution):\n",
    "    _, numPixels = np.unique(classification, return_counts=True)\n",
    "    maparea = numPixels * resolution ** 2 / 1000000\n",
    "    conf = 1.96\n",
    "    A = np.nansum(maparea) \n",
    "    W_i = maparea / A\n",
    "    n_i = np.nansum(confmat, axis=1)\n",
    "    p = (W_i * confmat.T / n_i).T\n",
    "    p[np.isnan(p)] = 0\n",
    "    p_area = np.nansum(p, axis=0) * A\n",
    "    p_area_CI = conf * A * np.sqrt( np.nansum( ((W_i * p.T - p.T ** 2) / (n_i - 1)).T , axis=0) )\n",
    "    OA = np.nansum(np.diag(p))\n",
    "    PA = np.diag(p) / np.nansum(p, axis=0)\n",
    "    UA = np.diag(p) / np.nansum(p, axis=1)\n",
    "    OA_CI = conf * np.sqrt(np.nansum(W_i ** 2 * UA * (1 - UA) / (n_i - 1)))\n",
    "    UA_CI = conf * np.sqrt(UA * (1 - UA) / (n_i - 1))\n",
    "    N_j = np.array([np.nansum(maparea / n_i * confmat[: , x]) for x in range(len(maparea))])\n",
    "    tmp = np.array([np.nansum(np.delete(maparea, x) ** 2 * np.delete(confmat[:, x], x) / np.delete(n_i, x) * ( 1 - np.delete(confmat[:, x], x) / np.delete(n_i, x)) / (np.delete(n_i, x) - 1)) for x in range(len(maparea))])\n",
    "    PA_CI = conf * np.sqrt(1 / N_j ** 2 * (maparea ** 2 * ( 1 - PA ) ** 2 * UA * (1 - UA) / (n_i - 1) + PA ** 2 * tmp))\n",
    "    OAarray = np.empty(len(maparea)) * np.nan\n",
    "    OAarray[0] = OA\n",
    "    OA_CIarray = np.empty(len(maparea)) * np.nan\n",
    "    OA_CIarray[0] = OA_CI\n",
    "    results = np.around(np.vstack([p_area, p_area_CI, PA*100, PA_CI*100, UA*100, UA_CI*100, OAarray*100, OA_CIarray*100]).T, 2)\n",
    "    columnNames = [\"km2\", \"+-\", \"PA\", \"+-\", \"UA\", \"+-\", \"OA\", \"+-\"]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION METHOD 1 (IMAGE COMP)\n",
    "classif = gdal.Open(os.path.join(outDir, area + \"_classification.tif\")).ReadAsArray()\n",
    "classif = classif[16:-16, 16:-16]\n",
    "\n",
    "testmap = gdal.Open(os.path.join(outDir, os.path.basename(shpTe[:-4]) + \".tif\")).ReadAsArray()\n",
    "testmap = testmap[16:-16, 16:-16]\n",
    "\n",
    "testmapX = np.zeros(testmap.shape)\n",
    "testmapX[:] = np.nan\n",
    "\n",
    "for idx, i in enumerate(np.unique(testmap)):\n",
    "    if i != 0:\n",
    "        testmapX[testmap == i] = idx - 1\n",
    "\n",
    "pos = np.argwhere(~np.isnan(testmapX))\n",
    "mod = []\n",
    "ref = []\n",
    "for x, y in pos:\n",
    "    ref.append(int(testmapX[x, y]))\n",
    "    mod.append(int(classif[x, y]))\n",
    "    \n",
    "normalMat = np.zeros((len(np.unique(ref)), len(np.unique(ref))), dtype = np.int)\n",
    "for i in range(len(ref)):\n",
    "    normalMat[mod[i], ref[i]] += 1\n",
    "\n",
    "print(\"Method1\", np.sum(np.diag(normalMat) / np.sum(normalMat)) * 100)\n",
    "np.savetxt(os.path.join(outDir, area+\"_test_method1.txt\"), normalMat, fmt='%f', delimiter=\"\\t\")\n",
    "\n",
    "gg = areaAdjAcc(normalMat, classif, classes, 10)\n",
    "print(\"Method1olo\", gg[0,-2])\n",
    "np.savetxt(os.path.join(outDir, area+\"_test_method1_olo.txt\"), gg, fmt='%f', delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}